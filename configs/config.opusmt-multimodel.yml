####
# Example of a multiteacher config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: opusmt-multimodel-prod
  src: en
  trg: sv
  src_three_letter: eng
  trg_three_letter: swe

  #OPUS models are not ensembled, they have different vocabs anyway
  teacher-ensemble: 1

  #URL to the OPUS-MT model to use as the teacher
  opusmt-teacher:
    - "https://object.pouta.csc.fi/Tatoeba-MT-models/gem-gem/opus-2020-10-04.zip"
    - "https://object.pouta.csc.fi/Tatoeba-MT-models/eng-swe/opus+bt-2021-04-14.zip"

  #You only need to specify this if any model is target-multilingual
  target-language-token: "swe"
    
  #URL to the OPUS-MT model to use as the backward model
  opusmt-backward: "https://object.pouta.csc.fi/Tatoeba-MT-models/swe-eng/opus-2021-02-12.zip"

  #You only need to specify this if backward model is target-multilingual
  source-language-token: "eng"

  # path to a pretrained backward model (optional)
  backward-model: ""
  
  # limits per downloaded dataset
  mono-max-sentences-src: 10000000
  mono-max-sentences-trg: 2000000

  # parallel data limit (mainly used for downsizing testing runs, to make GPU steps quicker). 
  # Comment out or specify "inf" to use all parallel data
  # parallel-max-sentences: 10000
 
  # vocab training sample
  spm-sample-size: 10000000


  # split corpus to parallelize translation
  split-length: 2000000
  
  best-model: perplexity
  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      tc_Tatoeba-train-v2021-08-07: 0.3

marian-args:
# these configs override pipeline/train/configs
  training-backward:
    # change based on available training data
    after: 10e
  training-teacher-base:
    # remove for low resource languages or if training without augmentation
    after: 2e
# these configs override pipeline/translate/decoder.yml
  decoding-backward:
    # 12 Gb GPU, s2s model
    mini-batch-words: 2000
    beam-size: 12
  decoding-teacher:
    # 12 Gb GPU, ensemble of 2 teachers
    mini-batch-words: 1000
    # 2080ti or newer
    precision: float16
    mini-batch: 512

datasets:
  # parallel training corpus
  train:
    - tc_Tatoeba-Challenge-v2021-08-07 
  # datasets to merge for validation while training
  devtest:
    - tc_Tatoeba-Challenge-v2021-08-07
  # datasets for evaluation
  test:
    - tc_Tatoeba-Challenge-v2021-08-07
  mono-src:
    - news-crawl_news.2020
