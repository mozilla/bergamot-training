####
# Example of a multiteacher config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: opusmt-multimodel-test
  src: en
  trg: sv
  src_three_letter: eng
  trg_three_letter: swe

  #OPUS models are not ensembled, they have different vocabs anyway
  teacher-ensemble: 1

  #URL to the OPUS-MT model to use as the teacher
  opusmt-teacher:
    - "https://object.pouta.csc.fi/Tatoeba-MT-models/gem-gem/opus-2020-10-04.zip"
    - "https://object.pouta.csc.fi/Tatoeba-MT-models/eng-swe/opus+bt-2021-04-14.zip"

  #You only need to specify this if any model is target-multilingual
  target-language-token: "swe"
    
      #URL to the OPUS-MT model to use as the backward model
  #NOTE: this should probably be a different model from the teacher, teacher is used here just for testing
  opusmt-backward: "https://object.pouta.csc.fi/Tatoeba-MT-models/gem-gem/opus-2020-10-04.zip"

  #You only need to specify this if backward model is target-multilingual
  source-language-token: "eng"

  # path to a pretrained backward model (optional)
  backward-model: ""
  
  # limits per downloaded dataset
  mono-max-sentences-src: 10000
  mono-max-sentences-trg: 20000

  # parallel data limit (mainly used for downsizing testing runs, to make GPU steps quicker). 
  # Comment out or specify "inf" to use all parallel data
  parallel-max-sentences: 10000
  
  # if using very small parallel corpus, also remember to set a low spm size, otherwise
  # spm-encode fails due to not having enough subwords
  spm-vocab-size: 1000

  # split corpus to parallelize translation
  split-length: 2000000
  
  best-model: perplexity
  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      tc_Tatoeba-train-v2021-08-07: 0.5

marian-args:
  training-backward:
    disp-freq: 10
    save-freq: 100
    valid-freq: 100
    after: 500u
  training-teacher-base:
    disp-freq: 10
    save-freq: 100
    valid-freq: 100
    after: 500u
  training-teacher-finetuned:
    disp-freq: 10
    save-freq: 100
    valid-freq: 100
    after: 500u
  training-student:
    disp-freq: 10
    save-freq: 100
    valid-freq: 100
    after: 500u
  training-student-finetuned:
    disp-freq: 10
    save-freq: 100
    valid-freq: 100
    after: 500u
  decoding-backward:
    mini-batch-words: 2000
  decoding-teacher:
    mini-batch-words: 1000

#marian-args:
#  decoding-teacher:
    # 2080ti or newer
#    precision: float16

#TODO: extract this info straight from the OPUS model yml info file
datasets:
  # parallel training corpus
  train:
    - tc_Tatoeba-Challenge-v2021-08-07 
  # datasets to merge for validation while training
  devtest:
    - tc_Tatoeba-Challenge-v2021-08-07
  # datasets for evaluation
  test:
    - tc_Tatoeba-Challenge-v2021-08-07
  mono-src:
    - news-crawl_news.2020
