####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: opusmt
  src: en
  trg: fi
  src_three_letter: eng
  trg_three_letter: fin

  #OPUS models are not ensembled, they have different vocabs anyway
  teacher-ensemble: 1

  #URL to the OPUS-MT model to use as the teacher
  opusmt-teacher: "https://object.pouta.csc.fi/Tatoeba-MT-models/eng-fin/opusTCv20210807+bt-2021-09-01.zip"
  #URL to the OPUS-MT model to use as the backward model
  opusmt-backward: "https://object.pouta.csc.fi/Tatoeba-MT-models/fin-eng/opusTCv20210807+bt-2021-08-25.zip" 

  # path to a pretrained backward model (optional)
  backward-model: ""
  
  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000

  # split corpus to parallelize translation
  split-length: 2000000
  
  best-model: perplexity
  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      tc_Tatoeba-train-v2021-08-07.eng.fin: 0.5

marian-args:
  decoding-teacher:
    precision: float16
    mini-batch: 512

#TODO: extract this info straight from the OPUS model yml info file
datasets:
  # parallel training corpus
  train:
    - tc_Tatoeba-Challenge-v2021-08-07 
  # datasets to merge for validation while training
  devtest:
    - tc_Tatoeba-Challenge-v2021-08-07
  # datasets for evaluation
  test:
    - tc_Tatoeba-Challenge-v2021-08-07
