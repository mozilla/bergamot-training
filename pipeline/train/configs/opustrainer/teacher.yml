datasets:
  original: <dataset0> # Original parallel corpus
  backtranslated: <dataset1> # Back-translated data

stages:
  - pretrain
  - finetune

# Back-translated corpus can vary a lot in size, so we can try using original one to count epochs
pretrain:
  - original 0.6
  - backtranslated 0.4
  - until original 2

# Fine-tuning only on original clean corpus until the early stopping
finetune:
  - original 1.0
  - until original inf


modifiers:
- UpperCase: 0.07 # Apply randomly to 7% of sentences
- TitleCase: 0.05
- Typos: 0.05
- Noise: 0.0005
  min_word_length: 2 # Minimum word length for each word in the noisy sentence
  max_word_length: 5 # Maximum word length for each word in the noisy sentence
  max_words: 6 # Maximum number of words in each noisy sentence


# random seed should be different for different teacher models
seed: <seed>
num_fields: 2
